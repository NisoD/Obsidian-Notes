# Interpretability
How do we explain why models make their predicitons
## Causality 
We never observe causilty only correlation having said that
correlation doesn't prove causilty

> We will look at 4 topics in interpretability 
1) Analysis of training data
2) Static network analysis
3) Analysis of network dynamics
4) Mathematical analysis

## Analysis of training data influence
### Influence Functions
$Z_{test} = (x,y)$
Training Models with substracted data.
Important works:
- Influence functions as an approximation to quantify influence
- [[OlmoeTrace]] , [Paper](https://arxiv.org/pdf/2504.07096)
- [less](https://arxiv.org/pdf/2402.04333)
## Static Network Analysis
LLM's have various submodels
A research question is what do these submodels learn
### Probing LLMs
- We can build many encoder/decoder blocks
The input of such block is a vector for example GPT2 1600 Dim

### Bert Rediscovers the NLP pipline
[Paper](https://arxiv.org/pdf/1905.05950)
> Layers in Bert seem to learn different intermediate tasks
When we use classifier for examined property:
- we need a simple classifier as a complex classifier will 
provide additional noise that will alter the analysis.
Prof. Omri - Text length found to be a confounding factor.
## Analysis of network dynamics 
Examine work of network behavior during inference.
> One of the best in this work is Mor Geva TLV
#### Mor geva work

She had a work on what does a key learn looking at a Q and K 
And looking when does it trigger high values deriving what does 'turn it on'.
They look at top triggers for a specific layer.
Top 25 from 100M examples
[Mor Geva](https://scholar.google.com/citations?user=GxpQbSkAAAAJ&hl=en)
#### Another important mention is logit lens
What happens if we project with W tranpose before the end
*Saturation Event - The moment where the model knows it's chosen answer.*
[What is model actual prediction](https://arxiv.org/pdf/2410.20210)
The above works look at distribution of top chosen words
It shows that the model later shows the next top words to choose.
Work continue at looking can we map the embeddings to the task its working on.
> Can we cause the model to switch tasks?
Took an embeddings and ran it in another network.
##### What causes this phenomenon
- This work used **not trained** model with random init.
- 



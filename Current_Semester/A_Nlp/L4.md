### How do we evaluate a LLM?
> Why do we need evaluation?
- The objective of our loss doesn't correlate with MCQ performance
#### Why is it so hard?
- Evaluation of text generation has many possible answers 
- How can we compare different answers?

#### Contamination during pre training
Moreover, retrieving a specific information out of datasets is hard.
> It is hard to see whether a model really makes new information.
#### Expensiveness is a trade of for evaluation
--
#### There are many tasks to evaluate an LLM on
Our assumption is that we want that every task possible to human will be done by a LLM
> i.e phonetic representation of a word.

## Popular LLM benchamarks
- Big-Bench
- SWE-Bench
 
> All models report their results on the benchmarks
Writing a task in a specific  manner can alter the results significantly.

Moran Mizrahi- Built a prompt paraphrase corpus.
- Took a benchmark and made via LLM's 175 paraphrase of the same task.
- Did a human evaluation of each paraphrase
> A prompt prefers a specific model

![](https://i.imgur.com/tuI2819.png)
The idea above is to induce prompt variance in order to achieve signifiant results
Similar work is of Melani Sclar - She wrote possible formatting that matters the most.

![](https://i.imgur.com/CxghmEF.png)
> Another work is ALzharani , Voronov Et Al.
### Multi Prompt Eval
Modernistic methodology to evaluate LLM's for diverse purposes.
##### Three Evaluations
- Maximum Metric, MaxP - [Link My Notes to Moran Paper](obsidian://open?vault=MyMind&file=Lab_Project%2FPapers_Hub%2FState%20of%20What%20Art%2FState%20of%20What%20Art)
All proposed approaches are expensive.
- A possible ok solution is to sample some paraphrases .
> BLEU is a irrelevant metric now-days.
A stochastic approach to benchmarking.
>Its practically impossible to isolate a specific factor to understand model behaviour.
This question concerns both academia and the industry to save compute.
- Another important mention is DOVE that calls for working with multi prompt evaluation.
Next week interpretability.




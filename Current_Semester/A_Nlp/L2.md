## Guest - Michael Hassid

### Deep Dive into Transformers

#### Attention is all you need

> A significant paper that marked the change of era from RNN's to Transformers
> What Components were introduced?

- Sandwich Transformers work by Ofir Press
- Examined Reordering the sub-layers in the current architecture
- Came to the formula that he proposed to do first self attention and then
  self attention and feed forward.
- Embeddings:
  $E_v \in \mathbb{R}^{V \times d}$
  A matrix of size Vocab Size x Embedding Dim
  To solve the problem of different size Sentences we use paddings.
  Output Embeddings
  $o = (x \cdot E_{V}^{T}) \in \mathbb{R}^{1 \times V}$
  press suggested using the same matrix for input and output
  it stuck for a few years but now as LLM grew larger it isn't used anymore.

> In practice we do multi _head_ attention for each head we have 3 Learned Matrices
> Q,V,W.
> Head dimension is smaller in multi head attention, the matrices are size $R_{d\times d}$

We later combine the different heads with concatenation.

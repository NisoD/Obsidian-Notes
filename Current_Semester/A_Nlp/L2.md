## Guest - Michael Hassid

### Deep Dive into Transformers

#### Attention is all you need

> A significant paper that marked the change of era from RNN's to Transformers
> What Components were introduced?

- Sandwich Transformers work by Ofir Press
- Examined Reordering the sub-layers in the current architecture
- Came to the formula that he proposed to do first self attention and then
  self attention and feed forward.
- Embeddings:
  $E_v \in \mathbb{R}^{V \times d}$
  A matrix of size Vocab Size x Embedding Dim
  To solve the problem of different size Sentences we use paddings.
  Output Embeddings
  $o = (x \cdot E_{V}^{T}) \in \mathbb{R}^{1 \times V}$
  press suggested using the same matrix for input and output
  it stuck for a few years but now as LLM grew larger it isn't used anymore.

> In practice we do multi _head_ attention for each head we have 3 Learned Matrices
> Q,V,W.
> Head dimension is smaller in multi head attention, the matrices are size $R_{d\times d}$

We later combine the different heads with concatenation.

#### Attention Alternatives

- Attention matrix is $O(n^2)$
  > For example 4,096 tokens the size of matrix is larger than 16M

##### The problem is called attention bottleneck

Approximations

- Some solutions - RFA Performer
- Linformer
- Fastformer
- RNT
- Infinite Attention

- ...

##### Multi Query Attention [GQA](https://www.ibm.com/think/topics/grouped-query-attention)

Instead of having unique K,V,Q we can share one pair of Keys & Values
$A=SoftMax(Q\cdot K^T)\in \mathbb{R}^{n \times n}$

##### Grouped Query Attention i.e Lllama 2

- Instead of using one pair of KV we have several of them.

##### Multi Latent Attention

- Came from deep seek
- In order to to reduce the KV cache size

#### Faster Implementation of Attention

##### Flash Attention

- Looks at GPU Memory, We want to maximize SRAM usage.
  > Attention has many different process we have that we do separately why not fuse the computation into one?
- This is commonly used now optimised more FlashAttention2, found that gpu threads weren't optimised.
- Then moved on to FlashAttention3 , implemented only on Nvidia H100 gpus.
  > Overlap computation, Block Quantization, block wise matmul soft max operations.
  > _Side note we can use flash attention 2 in google-collab._

##### Flex Attention

- Sliding window attention, uses similar to flash attention ideas.
- Used today as it is a more flexible implementations.

##### Ring attention

- Deals with long sequence over several GPU's
- TLDR - sends block of KV between GPUs , each one computes part of the attention
  > Parallelize the computation and communication.

##### Relative Bias

- relative positions can be plugged in

##### ALiBi

- Same as relative bias but with non-learned bias
- [ALiBi](https://arxiv.org/pdf/2108.12409)

##### RoPE

-

#### Layer Norm

- Done Pre multi head

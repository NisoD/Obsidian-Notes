---
Year: 2024
Link: https://arxiv.org/pdf/2406.01574
Status: Read
Topic:
  - LLM EVAL
PDF:
  - "[[MMLU.pdf]]"
---
# Main Takeaways

1. CoT reasoning achieves better results on MMLU-Pro Indicating that it requires more reasoning
2. MMLU-Pro shows better deviance between models make it more relevant than MMLU
3. For GPT-4o CoT improved performance by 19%
4. GPT-4o achieving near perfect scores in many benchmarks lights urgent need in new benchmarks

# Architecture

57 Sub categories and 14 broader categories

Reduce redundancy and focus on key areas.

Remove easy questions and mistakes of annotations

Use of 5-Shot CoT to evaluate models

### Answer Extraction

![[Attachments/image 18.png|image 18.png]]

## Best model results

![[Attachments/image 1 11.png|image 1 11.png]]

# Final Takeaways

==History and Psychology, models  
generally show a higher performance floor compared to reasoning-intensive discipline  
==

==the limitations of the multiple-choice  
format. This format may not capture the depth of comprehension and creative response generation as  
effectively as open-ended answers, which better reflect real-world scenario  
==
---
Status: To read
---
# Background

Preference Tuning RLHF

One of LLM challenges is precise instruction following.

Preference data

Underspecification and context,

How robust are LLM to contextual (killing somone, killing python command)

Open weight models - Llama , mistral Gemma

Open source -(also data) Pythima Llama360 Olmo

Persona based data making, asking GPT for prompts similar to this one given by this person (Synthethic Data creation)

DPO -

**Question**

1. Nemotron Paper?
2. Do all models tailor responses for benchmarks and how does it affect overall preformance
3. Learning to summnarize with human feedback paper
4. Bradley Terry Reward model

  

  

# Model

Add text here

# Results

Add text here

# Code

Add link to code here (if exists)